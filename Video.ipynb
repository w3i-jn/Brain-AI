{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a51e19f-aa73-4649-a604-f2ea2a6c4d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import openvino as ov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e446006f-244c-4f06-b343-e0abbf6ca0f3",
   "metadata": {},
   "source": [
    "# Webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19ea7169-15f6-4df6-8628-ca1d44f4bb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Main():\n",
    "    camera = cv2.VideoCapture(source)\n",
    "\n",
    "    while(True):\n",
    "\n",
    "        ret, frame = camera.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        cv2.imshow(\"Webcam\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xff == ord('q'):\n",
    "            break\n",
    "\n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a3492d-8f7e-436c-94fb-30584ff35843",
   "metadata": {},
   "source": [
    "# Face Detection with Webcam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23100f8-da14-4b72-8cb7-c77d23c4b3d9",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc58c267-6f02-4c5d-aed6-70045d3c4d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape [1,3,384,672]\n",
      "Output shape [1,1,200,7]\n",
      "Input shape [1,3,64,64]\n",
      "Output shape [1,5,1,1]\n",
      "Input shape [1,3,62,62]\n",
      "Output shape <bound method PyCapsule.output of <CompiledModel:\n",
      "inputs[\n",
      "<ConstOutput: names[data] shape[1,3,62,62] type: f32>\n",
      "]\n",
      "outputs[\n",
      "<ConstOutput: names[prob] shape[1,2,1,1] type: f32>,\n",
      "<ConstOutput: names[age_conv3, fc3_a] shape[1,1,1,1] type: f32>\n",
      "]>>\n"
     ]
    }
   ],
   "source": [
    "core = ov.Core()\n",
    "\n",
    "model_face = core.read_model(model='model/face-detection-adas-0001.xml')\n",
    "compiled_model_face = core.compile_model(model = model_face, device_name=\"CPU\")\n",
    "\n",
    "input_layer_face = compiled_model_face.input(0)\n",
    "output_layer_face = compiled_model_face.output(0)\n",
    "\n",
    "print(\"Input shape\", input_layer_face.shape)\n",
    "print(\"Output shape\", output_layer_face.shape)\n",
    "\n",
    "model_emo = core.read_model(model='model/emotions-recognition-retail-0003.xml')\n",
    "compiled_model_emo = core.compile_model(model = model_emo, device_name=\"CPU\")\n",
    "\n",
    "input_layer_emo = compiled_model_emo.input(0)\n",
    "output_layer_emo = compiled_model_emo.output(0)\n",
    "\n",
    "print(\"Input shape\", input_layer_emo.shape)\n",
    "print(\"Output shape\", output_layer_emo.shape)\n",
    "\n",
    "model_ag = core.read_model(model='model/age-gender-recognition-retail-0013.xml')\n",
    "compiled_model_ag = core.compile_model(model = model_ag, device_name=\"CPU\")\n",
    "\n",
    "input_layer_ag = compiled_model_ag.input(0)\n",
    "output_layer_ag = compiled_model_ag.output\n",
    "\n",
    "print(\"Input shape\", input_layer_ag.shape)\n",
    "print(\"Output shape\", output_layer_ag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c80fe4-4f8b-41b4-ad28-07444adc791c",
   "metadata": {},
   "source": [
    "### Pre-Process New Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afc0ad83-1010-4977-86cb-b618126b8494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(frame, input_layer_face):\n",
    "    N, input_channels, input_height, input_width = input_layer_face.shape\n",
    "    \n",
    "    resized_frame = cv2.resize(frame, (input_width,input_height))\n",
    "    transposed_frame = resized_frame.transpose(2, 0, 1)\n",
    "    input_frame = np.expand_dims(transposed_frame, 0)\n",
    "\n",
    "    return input_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5219401-2145-49bb-bd90-c2cc0beb2a0d",
   "metadata": {},
   "source": [
    "# Postprocess the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212f2753-4977-4d8b-89a3-2de604203be1",
   "metadata": {},
   "source": [
    "#### Find the Face Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6e4aed2-c628-4e8b-99a8-38db015a59a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_faceboxes(image, results, confidence_threshold):\n",
    "    results = results.squeeze()\n",
    "\n",
    "    scores = results[:,2]\n",
    "    boxes = results[:, -4:]\n",
    "\n",
    "    face_boxes = boxes[scores >= confidence_threshold]\n",
    "    scores = scores[scores >= confidence_threshold]\n",
    "\n",
    "    image_h, image_w, image_channels = image.shape\n",
    "    face_boxes = face_boxes*np.array([image_w, image_h, image_w, image_h])\n",
    "    face_boxes = face_boxes.astype(np.int64)\n",
    "\n",
    "    return face_boxes, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23780725-8b9b-47c8-871f-f2bdfcbfc47a",
   "metadata": {},
   "source": [
    "### Draw the Emotion/Age/Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c282746-5c80-40fe-a768-b55c76815c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_age_gender_emotion(face_boxes, frame):\n",
    "\n",
    "    EMOTION_NAMES = ['neutral', 'happy', 'sad', 'surprise', 'anger']\n",
    "\n",
    "    show_frame = frame.copy()\n",
    "\n",
    "    for i in range(len(face_boxes)):\n",
    "\n",
    "        xmin, ymin, xmax, ymax = face_boxes[i]\n",
    "        face = frame[ymin:ymax, xmin:xmax]\n",
    "        # --- emotion ---\n",
    "        input_frame = preprocess(face, input_layer_emo)\n",
    "        results_emo = compiled_model_emo([input_frame])[output_layer_emo]\n",
    "        \n",
    "        results_emo = results_emo.squeeze()\n",
    "        index = np.argmax(results_emo)\n",
    "        # --- emotion ---\n",
    "        \n",
    "\n",
    "        # --- age and gender ---\n",
    "        input_image_ag = preprocess(face, input_layer_ag)\n",
    "        results_ag = compiled_model_ag([input_image_ag])\n",
    "        age, gender = results_ag[1], results_ag[0]\n",
    "        age = np.squeeze(age)\n",
    "        age = int(age*100)\n",
    "\n",
    "        gender = np.squeeze(gender)\n",
    "\n",
    "        if (gender[0]>=0.65):\n",
    "            gender = 'female'\n",
    "            box_color = (200, 200, 0)\n",
    "        \n",
    "        elif (gender[1]>=0.55):\n",
    "            gender = 'male'\n",
    "            box_color = (0, 200, 200)\n",
    "    \n",
    "        else:\n",
    "            gender = \"unknown\"\n",
    "            box_color = (200, 200, 200)\n",
    "\n",
    "        # --- age and gender ---\n",
    "\n",
    "        fontScale = frame.shape[1]/750\n",
    "\n",
    "\n",
    "        text = gender + ' ' + str(age) + ' ' + EMOTION_NAMES[index]\n",
    "        cv2.putText(show_frame, text, (xmin, ymin), cv2.FONT_HERSHEY_SIMPLEX, fontScale, (0, 200, 0), 2)\n",
    "        cv2.rectangle(img=show_frame, pt1=(xmin,ymin), pt2=(xmax,ymax), color=box_color, thickness=2)\n",
    "\n",
    "\n",
    "    return show_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8994677e-bd74-4e9c-9d9a-869296bf1e80",
   "metadata": {},
   "source": [
    "# Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1478ccd7-9056-42b4-9a89-dbe6dd9bbef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Main():\n",
    "    camera = cv2.VideoCapture(source)\n",
    "\n",
    "    while(True):\n",
    "\n",
    "        ret, frame = camera.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        input_image = preprocess(frame, input_layer_face)\n",
    "        results = compiled_model_face([input_image])[output_layer_face]\n",
    "        \n",
    "        face_boxes, scores = find_faceboxes(frame, results, confidence_threshold)\n",
    "        show_frame = draw_age_gender_emotion(face_boxes, frame)\n",
    "\n",
    "        cv2.imshow(\"Webcam\", show_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xff == ord('q'):\n",
    "            break\n",
    "\n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae747c3c-035a-4a8e-a4ab-38cc3f4db143",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m source \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 5\u001b[0m     Main()\n",
      "Cell \u001b[1;32mIn[7], line 12\u001b[0m, in \u001b[0;36mMain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     11\u001b[0m input_image \u001b[38;5;241m=\u001b[39m preprocess(frame, input_layer_face)\n\u001b[1;32m---> 12\u001b[0m results \u001b[38;5;241m=\u001b[39m compiled_model_face([input_image])[output_layer_face]\n\u001b[0;32m     14\u001b[0m face_boxes, scores \u001b[38;5;241m=\u001b[39m find_faceboxes(frame, results, confidence_threshold)\n\u001b[0;32m     15\u001b[0m show_frame \u001b[38;5;241m=\u001b[39m draw_age_gender_emotion(face_boxes, frame)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\OpenVINO101_Project\\Lib\\site-packages\\openvino\\runtime\\ie_api.py:365\u001b[0m, in \u001b[0;36mCompiledModel.__call__\u001b[1;34m(self, inputs, share_inputs, share_outputs, decode_strings)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_request \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    363\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_infer_request()\n\u001b[1;32m--> 365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_request\u001b[38;5;241m.\u001b[39minfer(\n\u001b[0;32m    366\u001b[0m     inputs,\n\u001b[0;32m    367\u001b[0m     share_inputs\u001b[38;5;241m=\u001b[39mshare_inputs,\n\u001b[0;32m    368\u001b[0m     share_outputs\u001b[38;5;241m=\u001b[39mshare_outputs,\n\u001b[0;32m    369\u001b[0m     decode_strings\u001b[38;5;241m=\u001b[39mdecode_strings,\n\u001b[0;32m    370\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\OpenVINO101_Project\\Lib\\site-packages\\openvino\\runtime\\ie_api.py:132\u001b[0m, in \u001b[0;36mInferRequest.infer\u001b[1;34m(self, inputs, share_inputs, share_outputs, decode_strings)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minfer\u001b[39m(\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     57\u001b[0m     inputs: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     61\u001b[0m     decode_strings: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     62\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OVDict:\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Infers specified input(s) in synchronous mode.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    Blocks all methods of InferRequest while request is running.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    :rtype: OVDict\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OVDict(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39minfer(_data_dispatch(\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    134\u001b[0m         inputs,\n\u001b[0;32m    135\u001b[0m         is_shared\u001b[38;5;241m=\u001b[39mshare_inputs,\n\u001b[0;32m    136\u001b[0m     ), share_outputs\u001b[38;5;241m=\u001b[39mshare_outputs, decode_strings\u001b[38;5;241m=\u001b[39mdecode_strings))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "confidence_threshold = .95\n",
    "source = 0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0857580b-4793-42a8-8764-da6e3eec2abf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
